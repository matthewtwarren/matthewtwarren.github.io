[{"content":"","date":null,"permalink":"/","section":"Matt Warren","summary":"","title":"Matt Warren"},{"content":"A collection of articles, recommendations and ramblings on a variety of topics spanning science to technology, and everything in between.\n","date":null,"permalink":"/posts/","section":"Posts","summary":"A collection of articles, recommendations and ramblings on a variety of topics spanning science to technology, and everything in between.","title":"Posts"},{"content":" Why cryopreservation matters: A researcher\u0026rsquo;s perspective. A woman wakes to find that she’s trapped inside a cryogenic chamber. She has no recollection of who she is or how she got there. It’s been decades since her last breath.\nThe notion of “cryogenically” freezing someone to preserve their body far into the future has long been a science-fiction staple: in this case, I’m describing the opening scene of the 2021 Netflix thriller “Oxygen”, although I could be referencing “Alien”, “Futurama”, or a number of other TV classics. Far from fiction, however, the need to store human cells is a genuine concern for biomedical research and, increasingly, for society too.\nWhen “cryogenics” is mentioned in popular media, what’s usually meant is cryopreservation – the scientific process of using sub-zero temperatures to preserve living organisms or biological materials. At these low temperatures (often approaching –200ºC) chemical reactions are drastically slowed, putting metabolism and natural aging processes on pause. When the need arises, the frozen material can then be warmed and its function restored without experiencing damage or decay. At least, that’s the idea.\nThis is an essential step used in many current medical treatments to extend the shelf-life of chemical ingredients or cells. Vaccines, including those which prevent COVID-19, are shipped and stored in this way. The freezing of reproductive cells like eggs and sperm is also a common method for preserving fertility. But it’s becoming even more important as scientific advancements change the way we treat diseases.\nWhile traditional drugs can be manufactured in pill-form and stored in a medicine cabinet for months, new treatments for diseases including cancer involve fragile materials like cells or antibodies, requiring a more considered approach. Because they’re not stable for long periods of time outside of the body, cryopreservation is often required to transport these materials between patients and medical facilities. Unfortunately, things are rarely as easy as they appear on TV, and cryopreservation is no different. The underlying process is complex and fraught with danger, meaning that most things don’t make it out of a deep freeze in one piece. The source of all the problems: ice.\nWhen biological matter is cooled below zero, ice formation is almost inevitable. Ice first appears as tiny crystals which then amass and grow over time. If you have a sweet tooth, you might have experienced this: it’s the same process that gives ice cream an unpleasant, grainy texture if it’s left out for too long. It’s not good for cells either. Ice can draw out water from cells causing severe dehydration, as well as squashing internal structures and rupturing the membranes which hold a cell together. For larger tissues and organs, blood vessels may be punctured, and the supply of oxygen and essential nutrients cut off. The outcome is irreversible and deadly internal damage.\nThankfully, teams of researchers all over the world are working together on this, and my team is one of them. The aim of my PhD is to identify new chemical additives, known as cryoprotectants, that can control ice growth and reduce the injury it causes.\nI should point out that this strategy isn’t new. In fact, it’s been used for millennia, just not by humans. For many species living in harsh environments, cryoprotectants are their key to survival, enabling some truly remarkable feats. Take the Canadian wood frog, or Rana Sylvatica. While most animals seek refuge from winter’s chill in warmer spaces like burrows or nests, this wood frog has a different strategy. It spends the winter frozen, literally. As the temperature drops, ice fills the frogs’ internal cavities; their hearts stop beating and they stop breathing. For months they appear, as one researcher describes, like “hard icy stones carved in the shape of a frog”. Yet, when spring arrives the frogs waste no time. Within hours they thaw, perfectly unharmed, and hop away in search of a mate or food. They’ve cracked cryopreservation.\nTo accomplish this, the frogs accumulate cryoprotectants in their blood before entering their “popsicle” phase. Some of these cryoprotectants are natural antifreezes that lower the freezing temperature and prevent vital fluids from turning to ice. Others have been found to stick to ice directly, preventing the growth of larger, more harmful ice crystals. It’s this particular trick that we’re copying and engineering into synthetic cryoprotectants that can be used to safely freeze human cells. The trouble is we don’t understand exactly how this works, so my project is trying to fill in the gaps.\nGiven everything I’ve described, you might have started to picture the lab I work in: an icy-cold room flanked with deep freezers and frogs peering out from glass tanks. At least, these were my friends’ impressions.\nThe room I call a lab doesn’t actually look that different from most offices: computers whirring away, a smell of coffee lingering in the air, emails announcing their arrival. Despite appearances, we’re doing the same thing as our experimental counterparts are doing – clad in goggles and gloves – just down the hall: running experiments, collecting data and testing hypotheses. Welcome to the field of computational chemistry.\nAs a computational chemist, much of my PhD involves creating experiments in a computer using molecular simulations, a tool that allows you to peer into the mysterious world of atoms and molecules, and study their precise movement over time. The technique has been around for a while, but with recent technological advancements we can now run more complex experiments with greater ease and accuracy.\nLike a virtual microscope, I’ve been using these simulations to understand and visualise how cryoprotectants can slow ice growth, mostly (thanks to COVID) from the comfort of my bedroom. Crucially, this computational work doesn’t happen in isolation, but in tandem with traditional experiments, reflecting the broader interdisciplinary approach that’s required to solve the most challenging scientific problems.\nOur work has revealed important insights that should aid the discovery of new cryoprotectants that can control ice growth more effectively. Our hope is that this translates into improved cryopreservation outcomes that will one day transform cell cryopreservation from science-fiction into science-fact.\n","date":"18 August 2023","permalink":"/posts/cryopreservation/","section":"Posts","summary":"Why cryopreservation matters: A researcher\u0026rsquo;s perspective","title":"Putting cells on ice: Science-facts not science-fiction"},{"content":"Projects I\u0026rsquo;ve released or contributed to.\n","date":null,"permalink":"/projects/","section":"Projects","summary":"Projects I\u0026rsquo;ve released or contributed to.","title":"Projects"},{"content":" Command-line tool for managing Spotify playlists, including filtering and sorting playlists by track tempo and key. Written in Python using the Spotipy library and the Spotify Web API, SortSpotify allows users to automate the organisation and sorting of their Spotify playlists using hidden data such as track tempo (BPM), key, and genre. You can also export saved playlist data as a CSV file for other uses, such as prining album labels for a physical (e.g. vinyl) collection (an additional feature I\u0026rsquo;ll be adding soon!).\nThis is a command-line application run from a terminal window - but don\u0026rsquo;t let that put you off! It can be easily installed and set up by following the instructions below. You can also check out the code on GitHub.\nFeatures # Create new playlists or filter existing playlists by track tempo (BPM) and key (Camelot values) Sort playlists in ascending or descending order by track tempo (BPM) Save playlist data as a spreadsheet (.csv file) Requirements # Python (version 3.x) Spotipy Installation # Clone the repository git clone https://github.com/matthewtwarren/SortSpotify.git Install the requirements cd SortSpotify pip install -r requirements.txt Setup # Head over to Spotify for Developers and create an app Open the app dashboard and find the Client ID and Client Secret In the app settings, add https://localhost:8080 to your Redirect URIs Open config.txt and add your Spotify username, Client ID and Client Secret Usage #SortSpotify can be used as a command line tool\nOpen your terminal, navigate to the SortSpotify directory and run: python -m src.run If it\u0026rsquo;s your first time running the code, you will need to authenticate the app and give it permission to access your Spotify data. An internet browser tab should open automatically to do this.\nOnce started, commands can be executed by entering the name of a command. A list of commands can be found by typing HELP SDJ\u0026gt; create playlist SDJ\u0026gt; sort playlist Please also note: playlists are not automatically added to your Spotify library when they are created using this tool. To add a playlist to your library, run:\nSDJ\u0026gt; add to spotify Examples # Below is an example demonstrating how to create a playlist, add tracks to the playlist, filter the playlist to only contain tracks with BPM between 115 and 125, before then adding the playlist to your Spotify library Welcome to SortSpotify: A command-line interface for managing and creating Spotify playlists. Type HELP for list of available commands or EXIT to terminate. SDJ\u0026gt; create playlist Enter a name for your playlist: House 110-120 BPM Playlist House/Disco 110-120 BPM created. SDJ\u0026gt; add to playlist Your Spotify playlists: 1 Discover Weekly 2 Release Radar 3 House 4 Disco 5 Jazz 6 Soul 7 Hip Hop 8 Garage 9 Techno 10 Alternative Enter the number(s) of the playlists to be added (e.g. 0,1,3) or ALL: 3,4 Tracks from House added to House/Disco 115-125 BPM Tracks from Disco added to House/Disco 115-125 BPM SDJ\u0026gt; filter playlist How would you like to filter the playlist (by tempo or key)?: tempo Please enter a BPM range (e.g. 110:120): 115:125 Tracks filtered by tempo. Playlist now contains 202 tracks. SDJ\u0026gt; add to spotify Playlist added to your Spotify library. SDJ\u0026gt; exit SortSpotify has terminated. ","date":null,"permalink":"/projects/sort_spotify/","section":"Projects","summary":"Command-line tool for managing Spotify playlists, including filtering and sorting playlists by track tempo and key.","title":"SortSpotify"},{"content":" Data overload is a uniquely 21st century problem. Could the answer be as old as life itself? The amount of data generated in today’s digital era is truly astounding. In the last two years alone more data was generated than in the previous millennia, and our hunger for data is showing no signs of letting up.\nOur current infrastructure and hardware will soon be unable to accommodate the incoming deluge of information, which is otherwise expected to consume all microchip grade silicon by 2040. To address this uniquely modern problem, industries are looking to an ancient alternative: DNA.\nThe potential of DNA as a storage medium has long been touted by scientists: its compactness, ease of replication and longevity seen as the clear advantages. This shouldn\u0026rsquo;t come as a shock, remembering that DNA has literally been the storage medium for all life on earth for probably as long as it has existed, however our ability to manipulate DNA in such a way to take advantage of these features is a far more recent discovery.\nAdvances over the last few decades in our ability to synthesise (write) and sequence (read) DNA have meant that this concept is now a reality; all kinds of media including Shakespeare’s sonnets, Deep Purple’s \u0026ldquo;Smoke on the Water\u0026rdquo; and a GIF of a galloping horse have all been successfully stored and retrieved from the seemingly invisible DNA strands. So how does this all work?\nDNA encodes information through its constituent parts. Each DNA molecule is a double-stranded polymer composed of four nucleotides: A (adenine) and T (thymine) structurally referred to as purines; C (cytosine) and G (guanine), the pyrimidines. In nature, three of these bases —known as a codon— encodes for a single amino acid, which in turn form the proteins which constitute life.\nIn more data-minded terms, being one of four possible nucleotides, each nucleotide represents a maximum of two bits of information. For reference: eight bits make up a byte; one thousand (well, 1024 to be precise) bytes make a kilobyte; another thousand kilobytes is a megabyte; and a thousand megabytes is a gigabyte. A 300 page book corresponds to around 400 kilobytes, an average .mp3 track is around 4 megabytes, and HD films typically run to 5 gigabytes. Because DNA is so compact, a single gram of it has a theoretical capacity of 215 million gigabytes, which corresponds to about 600 trillion books. This means that if stored as DNA, all of humanity\u0026rsquo;s current data could be contained within a single room.\nDNA storage. The structure of DNA, showing the four nucleotides (left) , and how they could encode binary information (right). DNA may also alleviate another issue plaguing data storage: hardware obsolescence. While storage devices and associated hardware tend to become obsolete over time —think floppy discs and CDs— it is unlikely that we will ever lose interest in sequencing DNA. It also won’t degrade as quickly as magnetic tape or CDs - the devices currently being used to house most digital archives - especially if stored under optimal conditions.\nThe main problem, of course, is cost. Although synthesising and sequencing DNA has become far less expensive in recent years, it still represents the biggest barrier to commercial use of the technology. For example, when Yaniv Erlich and Dina Zielinski of Columbia Univerity and the New York Genome Centre stored and retrieved a number of files using DNA in 2017, it cost them approximately $5000 USD per megabyte.\nDina Zielinski, a bioinformatics expert, explains why DNA storage is needed and how it could work. Video: TED.com While these costs are showing exponential decreases over time, some organisations are taking a more direct route to reducing costs. DNA storage start-up Catalog is rethinking the entire process by decoupling the process of synthesising DNA and encoding data.\nIn traditional methods, the information is encoded, through 1s and 0s, as a sequence of bases in each strand. Instead, Catalog’s idea is to use a relatively small number of prefabricated DNA fragments, each no longer 30 bases, which can then be combined in an almost infinite number of ways to encode information by using enzymatic reactions. This would reduce the amount of DNA synthesis required: the most expensive and time-consuming part.\nCatalog hope that this approach should enable the company to achieve costs competitive with other storage devices in a matter of years. Still, there are a number of other engineering challenges which must be solved before this technology is adopted commercially.\nIn the short term, it is more likely that we’ll see DNA being used for rarely-accessed archives, or by industries such as film studios, research laboratories or government agencies which need to store large amounts of data indefinitely. Given more time, however, DNA may become a key part of our infrastructure, storing not only the blueprint for life but all of the data that comes along with it.\n","date":"21 March 2022","permalink":"/posts/dna_storage/","section":"Posts","summary":"Data overload is a uniquely 21st century problem. Could the answer be as old as life itself?","title":"How DNA could solve the world's data storage problem"},{"content":" Understanding the 18th-century maths theorem that governs the reliability of (corona)virus testing. Picture this. You take a test for a rare disease and the result comes back positive. The test is also very accurate, giving the correct result 99% of the time. What is the chance you do actually have the disease? 99%, surely?\u0026hellip;\nBased on this information alone, you actually have no idea: there simply isn’t enough information to make a judgement.\nAs governments and institutions around the world ramp up their use of lateral flow tests (LFTs) for asymptomatic COVID-19 testing, such events are playing out more and more frequently, and this seemingly benign question is becoming increasingly important.\nThe rationale behind asymptomatic testing is simple: by taking a test just before entering certain shared spaces such as schools, theatres, or nightclubs, you can increase the level of confidence that you, or someone else, does not have the disease and therefore won’t be spreading it.\nHowever, there has been some concern over the accuracy of LFTs and the consequences of their results. One concern is that they could miss a significant number of positive cases because they are less sensitive than the slower, lab-based PCR test.\nAnother concern is quite the opposite – that the result could be positive when the person is actually disease-free – producing a so-called “false positive”. This might not sound like a terrible problem to have, but the impacts of false positives are significant: unnecessary stress; avoidable follow-up procedures; inability to work; and financial losses to name a few. It would really help to know how likely false positives are.\nThis brings us back to the original question: given a positive result from an LFT, what is the probability that the test is accurate, and you do in fact have the disease. The answer depends on one key piece of additional information and can be calculated using a simple but profound mathematical formula known as Bayes’ theorem.\nNamed after its inventor, the 18th-century mathematician and minister Thomas Bayes, Bayes’ theorem is a method of determining the probability of an event based on the best available evidence, also known as conditional probability. It provides a way to update an existing belief, such as a prediction or theory, given new or additional information.\nThe formula for Bayes’ theorem is given below, but the mathematical notation might need some explaining. \\(P\\) stands for probability, so \\(P(A)\\) represents the probability of event \\(A\\) occurring, and \\(P(B)\\) is the probability of event \\(B\\). \\(P(A | B)\\) means the conditional probability that event \\(A\\) occurs given that event \\(B\\) happened.\n\\(P(A | B) = \\frac{P(B | A) \\times P(A)}{P(B)}\\)\nKnowing this formula, let’s reframe our initial question in terms of events and known probabilities. We want to know the probability that someone has the disease, given a positive test result, which we can denote as P(A | B), where P(A) and P(B) are the probabilities of having the disease and getting a positive result, respectively.\nWe can assume the probability of having the disease, P(A), to be equal to the background rate of the disease in the population, for now let’s say 1 in 1000, or 0.001. Here, P(A) is also called the “prior probability”. This assumption is valid given that asymptomatic testing is used to screen large numbers of people who are healthy and showing no signs of the disease.\nThe UK government states that the false-positive rate for LFTs is less than 1 in 1000, and therefore the test correctly identifies 99% of people who are COVID-free. We’ll assume the test is equally as good at detecting positive cases, meaning that P(B | A) – the probability of testing positive if you have the disease – would be 0.99.\nThe final probability to work out is P(B), which is the probability of getting a positive result. To calculate this, we have to consider two events as there are two scenarios which can lead to a positive result: when you have the disease (i.e. a true positive) and, conversely, when you don’t (i.e. a false positive).\nThe probability of a true positive is 0.99 x 0.001 = 0.00099, and for a false positive it is 0.01 x 0.999 = 0.0099. Summing these, we get the total probability of a positive result: 0.011.\nWhen you plug all these values into the equation, Bayes\u0026rsquo; theorem yields the surprising, but nevertheless correct answer of 9%. There is a less than 1/10 chance you have the disease after testing positive, given all the conditions previously described.\nTo break this down, let’s look at some more relatable numbers. Take a population of 100,000 people, chosen at random and then tested for the disease. Using the case numbers reported in May 2021, there would be approximately 100 cases of COVID, and for simplicity let’s say that 99 of these are correctly identified. For the 99,900 remaining people who are disease-free, the test will correctly identify 98,901, but this leaves 999 people who will receive a false positive – roughly ten times the number of actual cases.\nThe reason the numbers don’t seem to add up is because of the rarity of the disease. If we use the data from January this year (2021) when COVID was at the height of its second peak, an estimated 1/55 had the disease and a positive LFT would have been accurate 66% of the time. Fast-forward a few months to May, when only 1/1000 were COVID-positive, and as we’ve seen, the outcome is very different.\nBayes’ theorem shows us why we need to factor this information into our predictions, something which might almost seem more common sense than complex statistics. It is an extremely simple yet powerful proposition which has found its way into many areas of modern thinking, from physics to philosophy.\nUnfortunately, it’s also something that’s woefully misunderstood. In one study in 2013, researchers asked 5000 qualified doctors to give the probability that a patient had cancer, given that they received a positive result on a 90% accurate test, and the disease was found in 1% of the population. Even when presented with a multiple-choice answer, only a quarter of the participants identified the correct answer of “around 1/10”.\nIt doesn’t just affect the medical profession either. In a number of high-profile court cases, prosecutors have grossly miscalculated the probabilities of two events occurring, such as an innocent person’s DNA matching that found at the crime scene, leading to a host of wrongful convictions.\nThe “prosecutors’ fallacy” lies in assuming the probability of A given B is equal to B given A, ignoring Bayes’ rule. In one tragic case, solicitor Sally Clark was convicted for murdering her two sons after an expert witness falsely claimed that the chances of both her children dying of sudden infant death syndrome (SIDS) was 1 in 73 million. This statistic failed to take into account the prior probability that someone is a double murderer, and a number of other considerations such as genetic or environmental factors linked to SIDS. Thankfully, re-examination of flawed statistical evidence led to the conviction being overturned three years later.\nGoing back now to COVID testing, what do these results say about the reliability of LFTs? First, it\u0026rsquo;s essential to consider your prior probability: as the disease becomes rarer, a positive result carries a greater chance of being false. This doesn’t mean that rapid testing becomes ineffective as case numbers fall – after all, they’re still detecting cases that would be otherwise missed – just that we should be more aware of false positives and how to handle them.\nExtrapolating Bayes’ theorem also proves why taking a second test is so important if you get a positive result the first time round. In the earlier hypothetical scenario, given that the probability of you having disease was updated to 9% after the first positive result, the chance that you are COVID-positive after receiving a second positive test result is over 90%.\nUltimately, it\u0026rsquo;s useful to remember that the numbers are not always as they seem. Rapid lateral flow testing remains a powerful tool to control contagious disease, but as Bayes has shown, to obtain the strongest conclusions you have to consider all the evidence.\n","date":"21 May 2021","permalink":"/posts/bayes_theorem/","section":"Posts","summary":"Understanding the 18th-century maths theorem that governs the reliability of (corona) virus testing.","title":"Bayes' theorem: Making sense of COVID tests"},{"content":" Welcome to my website \u0026ndash; thanks for stopping by! 👋 My name is Matt and I\u0026rsquo;m a (Postdoctoral) Computational Researcher in the Department of Biochemistry at the University of Oxford. In collaboration with Boehringer Ingelheim, I work on developing data-driven models that predict protein-ligand binding affinities to accelerate drug discovery.\nAlongisde my work, I enjoy playing and mixing music, running and cycling on trails, and using technology to solve problems that make life easier and more enjoyable - check out my projects to see some examples of stuff I\u0026rsquo;ve been working on.\n","date":null,"permalink":"/about/","section":"Matt Warren","summary":"Welcome to my website \u0026ndash; thanks for stopping by!","title":"About"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]